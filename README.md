# HiSTalk: Hierarchical Speech Feature-based Landmark Displacements for 3D Talking Head Animation
The official repository of the paper [HiSTalk: Hierarchical Speech Feature-based Landmark Displacements for 3D Talking Head Animation](https://arxiv.org/abs/2404.01647)

<p align='center'>
  <b>
    <a href="">Paper</a>
    | 
    <a href="https://wangryhen.github.io/HSFTalk.github.io/">Project Page</a>
    |
    <a href="https://github.com/wangryhen/HSFTALK">Code</a> 
  </b>
</p> 

<!-- Colab notebook demonstration: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Egq0_ZK5sJAAawShxC0y4JRZQuVS2X-Z?usp=sharing) -->

  <p align='center'>  
    <img src='/paper_images/framework.svg' width='1000'/>
  </p>

Given a speech signal as input, our framework <strong>HiSTalk</strong> can generate realistic 3D talking heads through the Hierarchical Speech Features to Sparse Landmarks (<stromng>HSF2S</strong>) module and the Sparse Landmarks to Dense Landmarks Displacements (<strong>S2D</strong>) module.

## Video Demonstration
- Please click the project page.
<p align="center">
  <video width="720" height="400" controls>
    <source src="./data/HSFTalk.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
</p>


## TODO
- [x] **Release Arxiv paper.**
- [x] **Release Project Website.**
- [ ] **Release code. (Once the paper is accepted)**
- [ ] **Release Pre-trained Model. (Once the paper is accepted)**



## Citation	

```
@article{2024hsftalk,
  title={HSFTalk: Hierarchical Speech Feature-based Landmark Displacements for 3D Talking Head Animation},
  author={},
  journal={arXiv preprint arXiv:2404.01647},
  year={2024}
}
```

## Acknowledgement
<!-- Some code are borrowed from following projects:
* [SpeechFormer++](https://github.com/wyhsirius/LIA)
* [Learning Landmarks](https://github.com/OpenTalker/DPE)
* [EAT](https://github.com/yuangan/EAT_code)
* [PD-FGC](https://github.com/Dorniwang/PD-FGC-inference)
* [Wav2Lip](https://github.com/Rudrabha/Wav2Lip)
* [FOMM video preprocessing](https://github.com/AliaksandrSiarohin/video-preprocessing) -->

Some figures in the paper is inspired by:
* [SpeechFormer++](https://arxiv.org/pdf/2302.14638)
* [Learning Landmarks](https://arxiv.org/pdf/2306.01415)
* [S2D-Dec](https://arxiv.org/pdf/2105.07463)

The README.md template is borrowed from [EDTalk](https://github.com/tanshuai0219/EDTalk)


Thanks for these great projects.
=======
>>>>>>> (update)
# Nerfies

This is the repository that contains source code for the [Nerfies website](https://nerfies.github.io).

If you find Nerfies useful for your work please cite:
```
@article{park2021nerfies
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}
```

# Website License
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.


